{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CommunityPredictionsBatch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zephyris/discoba_alphafold/blob/main/CommunityPredictionsBatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0nT5Xki1QBF",
        "cellView": "form"
      },
      "source": [
        "#@title Batch AlphaFold predictions for TriTrypDB\n",
        "\n",
        "#@markdown This Colab notebook carries out protein structure predictions for proteins using just a gene ID from TriTrypDB.\n",
        "#@markdown It runs in a batch mode, automatically splitting up large proteins to manageable chunks and can process multiple proteins.\n",
        "\n",
        "#@markdown All predictions are  shared to a community database - this avoids re-predicting proteins.\n",
        "#@markdown It also means your predictions aren't lost if your session times out.\n",
        "#@markdown If you want to keep your predictions private you'll have to do the prediction manually with [one of my other Colab notebooks](https://github.com/zephyris/discoba_alphafold).\n",
        "\n",
        "#@markdown **Enter your TriTrypDB gene ID here.**\n",
        "#@markdown You can also paste a list of gene IDs, separated by spaces or commas.\n",
        "\n",
        "gene_ids = '' #@param {type:\"string\"}\n",
        "gene_ids = gene_ids.replace(\",\", \" \")\n",
        "gene_ids = '\\n'.join(gene_ids.split())+\"\\n\"\n",
        "f = open(\"queries.txt\", \"w\")\n",
        "f.write(gene_ids)\n",
        "f.close()\n",
        "\n",
        "#@markdown Press <kbd>Ctrl</kbd> + <kbd>F9</kbd> or select `Runtime` > `Run All` to carry out predictions.\n",
        "#@markdown You should also make sure a GPU is selected under `Runtime` > `Change runtime type` too!\n",
        "\n",
        "#@markdown It'll keep predicting until Google decides you've had enough computing time... they do donate it for free after all.\n",
        "#@markdown The more you interact with the page the longer that will be, so looking at progress genuinely helps, especially for big proteins.\n",
        "\n",
        "#@markdown If you get the `Runtime disconnected` message just press `Reconnect` then hit <kbd>Ctrl</kbd> + <kbd>F9</kbd> and run it again.\n",
        "#@markdown Unfortunately, if you only get the option to `Connect without GPU`, you'll have to wait - normally a day or so.\n",
        "\n",
        "#@markdown If you want to avoid timeouts you can get a [Colab Pro subscription](https://colab.research.google.com/signup), but that costs a monthly fee.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "import os.path\n",
        "import re\n",
        "import hashlib\n",
        "import urllib\n",
        "\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "#TriTrypDB_Gene_ID = 'Tb927.1.380' #@param {type:\"string\"}\n",
        "#jobname = TriTrypDB_Gene_ID\n",
        "#print(\"Predicting \"+TriTrypDB_Gene_ID)\n",
        "\n",
        "#Submit_to_community_resource = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown _Optional:_ If you'd like to have your efforts recorded put a username here. Feel free to work as a team and share a user name, just make sure you pick something unique! This'll get recorded in the prediction file as a thank you.\n",
        "user_name = '' #@param {type:\"string\"}\n",
        "\n",
        "# settings\n",
        "num_models = 5\n",
        "use_msa = True\n",
        "use_env = True\n",
        "use_amber = False\n",
        "use_templates = False\n",
        "homooligomer = 1\n",
        "max_length = 800\n",
        "\n",
        "input_dir = 'queries'\n",
        "result_dir = 'results'\n",
        "do_not_overwite_results = True\n",
        "\n",
        "print(\"Predicting these genes IDs:\")\n",
        "print(gene_ids.replace(\" \", \"\\n\"))\n",
        "print(\"Thank you for sharing your results with the community!\")\n",
        "if user_name!=\"\":\n",
        "  print(f\"'{user_name}' will be listed to say thanks :)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9BHlxQ_IXx6",
        "cellView": "form"
      },
      "source": [
        "#@title Get everything installed\n",
        "\n",
        "#@markdown This doesn't actually install anything on your computer, just on Google's server.\n",
        "\n",
        "%%bash -s $use_amber $use_msa $use_templates $jobname $use_env\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_MSA=$2\n",
        "USE_TEMPLATES=$3\n",
        "JOBNAME=$4\n",
        "USE_ENV=$5\n",
        "\n",
        "# From ColabFold batch\n",
        "if [ ! -f AF2_READY ]; then\n",
        "  # install dependencies\n",
        "  pip -q install biopython dm-haiku ml-collections py3Dmol\n",
        "  wget -qnc https://raw.githubusercontent.com/sokrypton/ColabFold/main/beta/colabfold.py\n",
        "  echo \"Installed ColabFold dependencies\"\n",
        "\n",
        "  # download model\n",
        "  if [ ! -d \"alphafold/\" ]; then\n",
        "    git clone https://github.com/deepmind/alphafold.git --quiet\n",
        "    (cd alphafold; git checkout 0bab1bf84d9d887aba5cfb6d09af1e8c3ecbc408 --quiet)\n",
        "    mv alphafold alphafold_\n",
        "    mv alphafold_/alphafold .\n",
        "    # remove \"END\" from PDBs, otherwise biopython complains\n",
        "    sed -i \"s/pdb_lines.append('END')//\" /content/alphafold/common/protein.py\n",
        "    sed -i \"s/pdb_lines.append('ENDMDL')//\" /content/alphafold/common/protein.py\n",
        "    echo \"Downloaded AlphaFold model\"\n",
        "  fi\n",
        "\n",
        "  # download model params (~1 min)\n",
        "  if [ ! -d \"params/\" ]; then\n",
        "    mkdir params\n",
        "    curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \\\n",
        "    | tar x -C params\n",
        "  fi\n",
        "  touch AF2_READY\n",
        "  echo \"Downloaded AlphaFold model parameters\"\n",
        "fi\n",
        "# download libraries for interfacing with MMseqs2 API\n",
        "if [ ${USE_MSA} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f MMSEQ2_READY ]; then\n",
        "    apt-get -qq -y update 2>&1 1>/dev/null\n",
        "    apt-get -qq -y install jq curl zlib1g gawk 2>&1 1>/dev/null\n",
        "    touch MMSEQ2_READY\n",
        "    echo \"Downloaded MMSeqs2 libraries\"\n",
        "  fi\n",
        "fi\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "    touch CONDA_READY\n",
        "    echo \"Downloaded Amber and dependencies\"\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign3=3.2.2 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "  echo \"Setup template search\"\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
        "  (cd /usr/local/lib/python3.7/site-packages; patch -s -p0 < /content/alphafold_/docker/openmm.patch)\n",
        "  wget -qnc https://git.scicore.unibas.ch/schwede/openstructure/-/raw/7102c63615b64735c4941278d92b554ec94415f8/modules/mol/alg/src/stereo_chemical_props.txt\n",
        "  mv stereo_chemical_props.txt alphafold/common/\n",
        "  touch AMBER_READY\n",
        "  echo \"Downloaded OpenMM for Amber\"\n",
        "fi\n",
        "\n",
        "# Discoba-specific\n",
        "# install HMMER\n",
        "if [ ! -f HMMER_READY ]; then\n",
        "  apt-get install hmmer > /dev/null 2>&1\n",
        "  touch HMMER_READY\n",
        "  echo \"Installed HMMER\"\n",
        "fi\n",
        "# download the custom Discoba database\n",
        "if [ ! -f DISCOBA_READY ]; then\n",
        "  if [ -d discoba ]; then\n",
        "    rm -r discoba\n",
        "  fi\n",
        "  mkdir discoba\n",
        "  cd discoba\n",
        "    curl http://wheelerlab.net/discobaStats.txt -s -o discobaStats.txt\n",
        "    curl http://wheelerlab.net/discoba.fasta.gz -s -L -o discoba.fasta.gz\n",
        "    gzip -d discoba.fasta.gz\n",
        "  cd ..\n",
        "  touch DISCOBA_READY\n",
        "  echo \"Downloaded Discoba database\"\n",
        "fi\n",
        "# install HH-suite\n",
        "if [ ! -f HHSUITE_READY ]; then\n",
        "  if [ -d hh-suite ]; then\n",
        "    rm -r hh-suite\n",
        "  fi\n",
        "  git clone https://github.com/soedinglab/hh-suite --quiet\n",
        "  touch HHSUITE_READY\n",
        "  echo \"Installed HH-suite\"\n",
        "fi\n",
        "# install IUPred\n",
        "if [ ! -f IUPRED_READY ]; then\n",
        "  curl https://raw.githubusercontent.com/zephyris/discoba_alphafold/main/scripts/iupredCut.py -s -o iupredCut.py\n",
        "  curl https://iupred.elte.hu/static/bin/iupred3.tar.gz -s -o iupred3.tar.gz\n",
        "  tar -xf iupred3.tar.gz\n",
        "  # pip install scipy\n",
        "  touch IUPRED_READY\n",
        "  echo \"Installed IUPred\"\n",
        "fi\n",
        "\n",
        "echo \"\"\n",
        "echo \"All the software is ready to go... let's get predictin'.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8upYxh4N3XsZ",
        "cellView": "form"
      },
      "source": [
        "#@title Prep proteins for predictions\n",
        "\n",
        "#@markdown This gets the protein sequences from TriTrypDB and preps them.\n",
        "\n",
        "#@markdown If structures are already available you'll see the links listed below.\n",
        "\n",
        "%%bash -s $max_length\n",
        "\n",
        "MAXLENGTH=$1\n",
        "\n",
        "# setup worklist\n",
        "if [ ! -d queries ]; then\n",
        "  mkdir queries\n",
        "else\n",
        "  rm queries/*\n",
        "fi\n",
        "\n",
        "# fetch sequences\n",
        "while read P; do\n",
        "  curl \"https://tritrypdb.org/tritrypdb/service/record-types/gene/searches/single_record_question_GeneRecordClasses_GeneRecordClass/reports/srt\" -s \\\n",
        "    --data-raw \"data=%7B%22searchConfig%22%3A%7B%22parameters%22%3A%7B%22primaryKeys%22%3A%22\"$P\"%2CTriTrypDB%22%7D%7D%2C%22reportConfig%22%3A%7B%22attachmentType%22%3A%22plain%22%2C%22endOffset3%22%3A0%2C%22type%22%3A%22protein%22%2C%22sourceIdFilter%22%3A%22genesOnly%22%2C%22upstreamAnchor%22%3A%22Start%22%2C%22upstreamSign%22%3A%22plus%22%2C%22upstreamOffset%22%3A0%2C%22downstreamAnchor%22%3A%22End%22%2C%22downstreamSign%22%3A%22plus%22%2C%22downstreamOffset%22%3A0%2C%22onlyIdDefLine%22%3A%220%22%2C%22noLineBreaks%22%3A%220%22%2C%22startAnchor3%22%3A%22Start%22%2C%22startOffset3%22%3A0%2C%22endAnchor3%22%3A%22End%22%7D%7D\" \\\n",
        "    > \"queries/\"$P\".fasta\"\n",
        "done < queries.txt\n",
        "\n",
        "# use IUPred to find disordered regions\n",
        "for P in queries/*.fasta; do\n",
        "  python iupred3/iupred3.py $P short | grep \"^[^#;]\" > $P\".iupred\"\n",
        "done\n",
        "\n",
        "# split FASTAs to jobs\n",
        "echo \"Assigned predictions for today:\"\n",
        "echo \"Protein FASTA files\"\n",
        "for P in queries/*.fasta; do\n",
        "  LENGTH=$(cat $P\".iupred\" | wc -l)\n",
        "  echo $P \"(\"$LENGTH\" aa)\"\n",
        "  if [ $LENGTH -gt $MAXLENGTH ]; then\n",
        "    rm $P\n",
        "    python3 iupredCut.py $P\".iupred\" $MAXLENGTH\n",
        "  else\n",
        "    mv $P ${P%.fasta}\"_1-\"$LENGTH\".fasta\"\n",
        "  fi\n",
        "done\n",
        "\n",
        "# remove IUPred results\n",
        "rm queries/*.iupred\n",
        "\n",
        "echo \"\"\n",
        "echo \"FASTA files split into workable chunks\"\n",
        "# summarise\n",
        "for P in queries/*.fasta; do\n",
        "  FILE=$(basename -- $P)\n",
        "  STATUS=$(curl -I -s \"http://www.wheelerlab.net/check.php?idse=\"${FILE%.fasta} | head -n1 | awk '{print $2}')\n",
        "  LENGTH=$(cat $P | tail -n+2 | sed 's/^[[:space:]]*//g' | wc -c)\n",
        "  if [ $STATUS = \"404\" ]; then\n",
        "    echo $P \"(\"$LENGTH\" aa)\"\n",
        "  else\n",
        "    rm $P\n",
        "    echo $P\" already predicted, removed from your workload!\"\n",
        "  fi\n",
        "done\n",
        "\n",
        "# remove FASTAs with identical sequences\n",
        "echo \"\"\n",
        "echo \"Checking for duplicate sequences\"\n",
        "for P in queries/*.fasta; do\n",
        "  for Q in queries/*.fasta; do\n",
        "    if [ $P != $Q ] && [ -f $P ] && [ -f $Q ]; then\n",
        "      PSEQ=$(cat $P | tail -n 1)\n",
        "      QSEQ=$(cat $Q | tail -n 1)\n",
        "      if [ $PSEQ == $QSEQ ]; then\n",
        "        echo $Q\" has the same sequence as \"$P\", removed from your workload!\"\n",
        "        rm $Q\n",
        "      fi\n",
        "    fi\n",
        "  done\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzJ67g3cHCSa",
        "cellView": "form"
      },
      "source": [
        "#@title Predict some structures!\n",
        "\n",
        "#@markdown Predictions are made one-by-one, starting with the shortest sequence.\n",
        "#@markdown As soon as each prediction finishes the results are automatically uploaded to the database server.\n",
        "#@markdown It sends absolutely no personal information, just the prediction result (and your user name, if you picked one).\n",
        "\n",
        "#@markdown Because this runs in batch mode there can be many sets of results files. \n",
        "#@markdown It's not really possible to download the results automatically - auto-downloading lots of files looks like a virus!\n",
        "#@markdown Instead, you can manually grab the results using the file browser on the interface (click on the little folder icon on the left).\n",
        "\n",
        "#@markdown Or... just follow the links to the database website!\n",
        "\n",
        "#@markdown Occasionally errors happen which stop the predictions (the little play button for this section goes red). Just hit <kbd>Ctrl</kbd> + <kbd>F9</kbd> to restart, and you can always select `Runtime` > `Factory reset runtime` to do a full reset of everything.\n",
        "\n",
        "print (\"You're now running AlphaFold!\")\n",
        "print (\"\")\n",
        "\n",
        "# setup the model\n",
        "if \"model\" not in dir():\n",
        "\n",
        "  # hiding warning messages\n",
        "  import warnings\n",
        "  from absl import logging\n",
        "  import os\n",
        "  import tensorflow as tf\n",
        "  warnings.filterwarnings('ignore')\n",
        "  logging.set_verbosity(\"error\")\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "  tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "  import sys\n",
        "  import numpy as np\n",
        "  import pickle\n",
        "  from alphafold.common import protein\n",
        "  from alphafold.data import pipeline\n",
        "  from alphafold.data import templates\n",
        "  from alphafold.model import data\n",
        "  from alphafold.model import config\n",
        "  from alphafold.model import model\n",
        "  from alphafold.data.tools import hhsearch\n",
        "  from alphafold.model.tf import shape_placeholders\n",
        "  NUM_RES = shape_placeholders.NUM_RES\n",
        "  NUM_MSA_SEQ = shape_placeholders.NUM_MSA_SEQ\n",
        "  NUM_EXTRA_SEQ = shape_placeholders.NUM_EXTRA_SEQ\n",
        "  NUM_TEMPLATES = shape_placeholders.NUM_TEMPLATES\n",
        "\n",
        "  import colabfold as cf\n",
        "  # plotting libraries\n",
        "  import py3Dmol\n",
        "  import matplotlib.pyplot as plt\n",
        "  import ipywidgets\n",
        "  from ipywidgets import interact, fixed, GridspecLayout, Output\n",
        "\n",
        "  import json\n",
        "\n",
        "if use_amber and \"relax\" not in dir():\n",
        "  sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
        "  from alphafold.relax import relax\n",
        "\n",
        "def mk_mock_template(query_sequence, num_temp=1):\n",
        "  ln = len(query_sequence)\n",
        "  output_templates_sequence = \"A\"*ln\n",
        "  output_confidence_scores = np.full(ln,1.0)\n",
        "  templates_all_atom_positions = np.zeros((ln, templates.residue_constants.atom_type_num, 3))\n",
        "  templates_all_atom_masks = np.zeros((ln, templates.residue_constants.atom_type_num))\n",
        "  templates_aatype = templates.residue_constants.sequence_to_onehot(output_templates_sequence,\n",
        "                                                                    templates.residue_constants.HHBLITS_AA_TO_ID)\n",
        "  template_features = {'template_all_atom_positions': np.tile(templates_all_atom_positions[None],[num_temp,1,1,1]),\n",
        "                       'template_all_atom_masks': np.tile(templates_all_atom_masks[None],[num_temp,1,1]),\n",
        "                       'template_sequence': [f'none'.encode()]*num_temp,\n",
        "                       'template_aatype': np.tile(np.array(templates_aatype)[None],[num_temp,1,1]),\n",
        "                       'template_confidence_scores': np.tile(output_confidence_scores[None],[num_temp,1]),\n",
        "                       'template_domain_names': [f'none'.encode()]*num_temp,\n",
        "                       'template_release_date': [f'none'.encode()]*num_temp}\n",
        "  return template_features\n",
        "\n",
        "\n",
        "def mk_template(a3m_lines, template_paths):\n",
        "  template_featurizer = templates.TemplateHitFeaturizer(\n",
        "      mmcif_dir=template_paths,\n",
        "      max_template_date=\"2100-01-01\",\n",
        "      max_hits=20,\n",
        "      kalign_binary_path=\"kalign\",\n",
        "      release_dates_path=None,\n",
        "      obsolete_pdbs_path=None)\n",
        "\n",
        "  hhsearch_pdb70_runner = hhsearch.HHSearch(binary_path=\"hhsearch\", databases=[f\"{template_paths}/pdb70\"])\n",
        "\n",
        "  hhsearch_result = hhsearch_pdb70_runner.query(a3m_lines)\n",
        "  hhsearch_hits = pipeline.parsers.parse_hhr(hhsearch_result)\n",
        "  templates_result = template_featurizer.get_templates(query_sequence=query_sequence,\n",
        "                                                       query_pdb_code=None,\n",
        "                                                       query_release_date=None,\n",
        "                                                       hits=hhsearch_hits)\n",
        "  return templates_result.features\n",
        "\n",
        "def make_fixed_size(protein, shape_schema, msa_cluster_size, extra_msa_size,\n",
        "                   num_res, num_templates=0):\n",
        "  \"\"\"Guess at the MSA and sequence dimensions to make fixed size.\"\"\"\n",
        "\n",
        "  pad_size_map = {\n",
        "      NUM_RES: num_res,\n",
        "      NUM_MSA_SEQ: msa_cluster_size,\n",
        "      NUM_EXTRA_SEQ: extra_msa_size,\n",
        "      NUM_TEMPLATES: num_templates,\n",
        "  }\n",
        "\n",
        "  for k, v in protein.items():\n",
        "    # Don't transfer this to the accelerator.\n",
        "    if k == 'extra_cluster_assignment':\n",
        "      continue\n",
        "    shape = list(v.shape)\n",
        "    \n",
        "    schema = shape_schema[k]\n",
        "\n",
        "    assert len(shape) == len(schema), (\n",
        "        f'Rank mismatch between shape and shape schema for {k}: '\n",
        "        f'{shape} vs {schema}')\n",
        "    pad_size = [\n",
        "        pad_size_map.get(s2, None) or s1 for (s1, s2) in zip(shape, schema)\n",
        "    ]\n",
        "    padding = [(0, p - tf.shape(v)[i]) for i, p in enumerate(pad_size)]\n",
        "\n",
        "    if padding:\n",
        "      protein[k] = tf.pad(\n",
        "          v, padding, name=f'pad_to_fixed_{k}')\n",
        "      protein[k].set_shape(pad_size)\n",
        "  return {k:np.asarray(v) for k,v in protein.items()}\n",
        "\n",
        "def set_bfactor(pdb_filename, bfac, idx_res, chains):\n",
        "  I = open(pdb_filename,\"r\").readlines()\n",
        "  O = open(pdb_filename,\"w\")\n",
        "  for line in I:\n",
        "    if line[0:6] == \"ATOM  \":\n",
        "      seq_id = int(line[22:26].strip()) - 1\n",
        "      seq_id = np.where(idx_res == seq_id)[0][0]\n",
        "      O.write(f\"{line[:21]}{chains[seq_id]}{line[22:60]}{bfac[seq_id]:6.2f}{line[66:]}\")\n",
        "  O.close()\n",
        "\n",
        "def predict_structure(prefix, feature_dict, Ls, crop_len, model_params, use_model, do_relax=False, random_seed=0):  \n",
        "  \"\"\"Predicts structure using AlphaFold for the given sequence.\"\"\"\n",
        "  idx_res = feature_dict['residue_index']\n",
        "  chains = list(\"\".join([ascii_uppercase[n]*L for n,L in enumerate(Ls)]))\n",
        "\n",
        "  # Run the models.\n",
        "  plddts,paes = [],[]\n",
        "  unrelaxed_pdb_lines = []\n",
        "  relaxed_pdb_lines = []\n",
        "  seq_len = feature_dict['seq_length'][0]\n",
        "  for model_name, params in model_params.items():\n",
        "    if model_name in use_model:\n",
        "      print(f\"running {model_name}\")\n",
        "      # swap params to avoid recompiling\n",
        "      # note: models 1,2 have diff number of params compared to models 3,4,5\n",
        "      if any(str(m) in model_name for m in [1,2]): model_runner = model_runner_1\n",
        "      if any(str(m) in model_name for m in [3,4,5]): model_runner = model_runner_3\n",
        "      model_runner.params = params\n",
        "\n",
        "      processed_feature_dict = model_runner.process_features(feature_dict, random_seed=random_seed)\n",
        "      model_config = model_runner.config\n",
        "      eval_cfg = model_config.data.eval\n",
        "      crop_feats = {k:[None]+v for k,v in dict(eval_cfg.feat).items()}     \n",
        "\n",
        "      # templates models\n",
        "      if model_name == \"model_1\" or model_name == \"model_2\":\n",
        "        pad_msa_clusters = eval_cfg.max_msa_clusters - eval_cfg.max_templates\n",
        "      else:\n",
        "        pad_msa_clusters = eval_cfg.max_msa_clusters\n",
        "\n",
        "      max_msa_clusters = pad_msa_clusters\n",
        "\n",
        "      # let's try pad (num_res + X) \n",
        "      input_fix = make_fixed_size(processed_feature_dict,\n",
        "                                  crop_feats,\n",
        "                                  msa_cluster_size=max_msa_clusters, # true_msa (4, 512, 68)\n",
        "                                  extra_msa_size=5120, # extra_msa (4, 5120, 68)\n",
        "                                  num_res=crop_len, # aatype (4, 68)\n",
        "                                  num_templates=4) # template_mask (4, 4) second value\n",
        "\n",
        "      prediction_result = model_runner.predict(input_fix)\n",
        "      unrelaxed_protein = protein.from_prediction(input_fix,prediction_result)\n",
        "      unrelaxed_pdb_lines.append(protein.to_pdb(unrelaxed_protein))\n",
        "      plddts.append(prediction_result['plddt'][:seq_len])\n",
        "      paes_res = []\n",
        "      for i in range(seq_len):\n",
        "        paes_res.append(prediction_result['predicted_aligned_error'][i][:seq_len])\n",
        "      paes.append(paes_res)\n",
        "      if do_relax:\n",
        "        # Relax the prediction.\n",
        "        amber_relaxer = relax.AmberRelaxation(max_iterations=0,tolerance=2.39,\n",
        "                                              stiffness=10.0,exclude_residues=[],\n",
        "                                              max_outer_iterations=20)      \n",
        "        relaxed_pdb_str, _, _ = amber_relaxer.process(prot=unrelaxed_protein)\n",
        "        relaxed_pdb_lines.append(relaxed_pdb_str)\n",
        "\n",
        "  # rerank models based on predicted lddt\n",
        "  lddt_rank = np.mean(plddts,-1).argsort()[::-1]\n",
        "  out = {}\n",
        "  print(\"reranking models based on avg. predicted lDDT\")\n",
        "  for n,r in enumerate(lddt_rank):\n",
        "    print(f\"model_{n+1} {np.mean(plddts[r])}\")\n",
        "\n",
        "    unrelaxed_pdb_path = f'{prefix}_unrelaxed_model_{n+1}.pdb'    \n",
        "    with open(unrelaxed_pdb_path, 'w') as f: f.write(unrelaxed_pdb_lines[r])\n",
        "    set_bfactor(unrelaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    if do_relax:\n",
        "      relaxed_pdb_path = f'{prefix}_relaxed_model_{n+1}.pdb'\n",
        "      with open(relaxed_pdb_path, 'w') as f: f.write(relaxed_pdb_lines[r])\n",
        "      set_bfactor(relaxed_pdb_path, plddts[r], idx_res, chains)\n",
        "\n",
        "    out[f\"model_{n+1}\"] = {\"plddt\":plddts[r], \"pae\":paes[r], \"unrelaxedPdb\":unrelaxed_pdb_lines[r]}\n",
        "    if do_relax:\n",
        "      out[f\"model_{n+1}\"][\"relaxedPdb\"]=relaxed_pdb_lines[r]\n",
        "  return out\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
        "queries = []\n",
        "for filename in onlyfiles:\n",
        "    extension = os.path.splitext(filename)[1]\n",
        "    jobname=os.path.splitext(filename)[0]\n",
        "    filepath = input_dir+\"/\"+filename\n",
        "    with open(filepath) as f:\n",
        "      input_fasta_str = f.read()\n",
        "    (seqs, header) = pipeline.parsers.parse_fasta(input_fasta_str)\n",
        "    query_sequence = seqs[0]\n",
        "    queries.append((jobname, query_sequence, extension))\n",
        "# sort by seq. len    \n",
        "queries.sort(key=lambda t: len(t[1]))\n",
        "import math\n",
        "crop_len = math.ceil(len(queries[0][1]) * 1.1)\n",
        "\n",
        "### run\n",
        "for (jobname, query_sequence, extension) in queries:\n",
        "  a3m_file = f\"{jobname}.a3m\"\n",
        "  if len(query_sequence) > crop_len:\n",
        "    crop_len = math.ceil(len(query_sequence) * 1.1)\n",
        "  print(\"Running: \"+jobname)\n",
        "  if do_not_overwite_results == True and os.path.isfile(result_dir+\"/\"+jobname+\".result.zip\"):\n",
        "    continue\n",
        "  if use_templates:\n",
        "    try:  \n",
        "      a3m_lines, template_paths = cf.run_mmseqs2(query_sequence, jobname, use_env, use_templates=True)\n",
        "    except:\n",
        "      print(jobname+\" cound not be processed\")\n",
        "      continue  \n",
        "    if template_paths is None:\n",
        "      template_features = mk_mock_template(query_sequence, 100)\n",
        "    else:\n",
        "      template_features = mk_template(a3m_lines, template_paths)\n",
        "    if extension.lower() == \".a3m\":\n",
        "      a3m_lines = \"\".join(open(input_dir+\"/\"+a3m_file,\"r\").read())\n",
        "  else:\n",
        "    if extension.lower() == \".a3m\":\n",
        "      a3m_lines = \"\".join(open(input_dir+\"/\"+a3m_file,\"r\").read())\n",
        "    else:\n",
        "      try:  \n",
        "        a3m_lines = cf.run_mmseqs2(query_sequence, jobname, use_env)\n",
        "      except:\n",
        "        print(jobname+\" cound not be processed\")\n",
        "        continue\n",
        "    template_features = mk_mock_template(query_sequence, 100)\n",
        "\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(a3m_lines)\n",
        "\n",
        "  #Add discoba\n",
        "  %shell head -n2 $a3m_file > tmp.fasta\n",
        "  %shell jackhmmer -A tmp.sto -o tmp.out tmp.fasta discoba/discoba.fasta\n",
        "  %shell perl hh-suite/scripts/reformat.pl sto a3m tmp.sto tmp.a3m -l 30000 2>&1 1>/dev/null\n",
        "  %shell cat tmp.a3m | tail -n+1 >> $a3m_file\n",
        "  with open(a3m_file, \"r\") as text_file:\n",
        "    a3m_lines=text_file.read()\n",
        "\n",
        "  # parse MSA\n",
        "  msa, deletion_matrix = pipeline.parsers.parse_a3m(a3m_lines)\n",
        "\n",
        "  #@title Gather input features, predict structure\n",
        "  from string import ascii_uppercase\n",
        "\n",
        "  # collect model weights\n",
        "  use_model = {}\n",
        "  if \"model_params\" not in dir(): model_params = {}\n",
        "  for model_name in [\"model_1\",\"model_2\",\"model_3\",\"model_4\",\"model_5\"][:num_models]:\n",
        "    use_model[model_name] = True\n",
        "    if model_name not in model_params:\n",
        "      model_params[model_name] = data.get_model_haiku_params(model_name=model_name+\"_ptm\", data_dir=\".\")\n",
        "      if model_name == \"model_1\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_1 = model.RunModel(model_config, model_params[model_name])\n",
        "      if model_name == \"model_3\":\n",
        "        model_config = config.model_config(model_name+\"_ptm\")\n",
        "        model_config.data.eval.num_ensemble = 1\n",
        "        model_runner_3 = model.RunModel(model_config, model_params[model_name])\n",
        "\n",
        "\n",
        "  msas = [msa]\n",
        "  deletion_matrices = [deletion_matrix]\n",
        "  try:\n",
        "  # gather features\n",
        "    feature_dict = {\n",
        "        **pipeline.make_sequence_features(sequence=query_sequence,\n",
        "                                          description=\"none\",\n",
        "                                          num_res=len(query_sequence)),\n",
        "        **pipeline.make_msa_features(msas=msas,deletion_matrices=deletion_matrices),\n",
        "        **template_features\n",
        "    }\n",
        "  except:\n",
        "    print(jobname+\" cound not be processed\")\n",
        "    continue\n",
        "\n",
        "  \n",
        "  outs = predict_structure(jobname, feature_dict,\n",
        "                           Ls=[len(query_sequence)], crop_len=crop_len,\n",
        "                           model_params=model_params, use_model=use_model,\n",
        "                           do_relax=use_amber)\n",
        "\n",
        "  # gather MSA info\n",
        "  deduped_full_msa = list(dict.fromkeys(msa))\n",
        "  msa_arr = np.array([list(seq) for seq in deduped_full_msa])\n",
        "  seqid = (np.array(list(query_sequence)) == msa_arr).mean(-1)\n",
        "  seqid_sort = seqid.argsort() #[::-1]\n",
        "  non_gaps = (msa_arr != \"-\").astype(float)\n",
        "  non_gaps[non_gaps == 0] = np.nan\n",
        "\n",
        "  # json out\n",
        "  result={}\n",
        "  for model_num in range(1,num_models+1):\n",
        "    model_name = f\"model_{model_num}\"\n",
        "    result[model_num]={}\n",
        "    result[model_num][\"plddt\"]=outs[model_name][\"plddt\"].tolist()\n",
        "    result[model_num][\"pae\"]=outs[model_name][\"pae\"]\n",
        "    for index in range(len(outs[model_name][\"pae\"])):\n",
        "      outs[model_name][\"pae\"][index]= outs[model_name][\"pae\"][index].tolist()\n",
        "    result[model_num][\"unrelaxedPDB\"]=outs[model_name][\"unrelaxedPdb\"].split(\"\\n\");\n",
        "    if \"relaxedPdb\" in outs[model_name]:\n",
        "      result[model_num][\"relaxedPDB\"]=outs[model_name][\"relaxedPdb\"].split(\"\\n\");\n",
        "\n",
        "  result[\"msa\"]={}\n",
        "  result[\"msa\"][\"coverage\"]=(msa_arr != \"-\").sum(0).tolist()\n",
        "  result[\"msa\"][\"alignment\"]=deduped_full_msa\n",
        "\n",
        "  result[\"id\"]=\"_\".join(jobname.split(\"_\")[:-1])\n",
        "  ends=jobname.split(\"_\")[-1].split(\"-\")\n",
        "  result[\"start\"]=int(ends[0])\n",
        "  result[\"end\"]=int(ends[1])\n",
        "  result[\"seq\"]=result[\"msa\"][\"alignment\"][0]\n",
        "  result[\"hash\"]=hashlib.sha1(result[\"seq\"].encode()).hexdigest()[:5]\n",
        "\n",
        "  from datetime import datetime\n",
        "  now=datetime.utcnow()\n",
        "  epoch=datetime.utcfromtimestamp(0)\n",
        "  result[\"metadata\"]={};\n",
        "  result[\"metadata\"][\"time\"]=datetime.strftime(now, \"%d/%m/%Y, %H:%M:%S\")\n",
        "  result[\"metadata\"][\"timeStamp\"]=(now - epoch).total_seconds()\n",
        "  result[\"metadata\"][\"predictedBy\"]=user_name\n",
        "  \n",
        "  with open(f\"{jobname}.json\", \"w\") as jsonfile:\n",
        "    json.dump(result, jsonfile, indent=2)\n",
        "\n",
        "  print(\"Submitting the prediction to the community-generated database\")\n",
        "  %shell gzip -k $jobname\".json\"\n",
        "  %shell curl -i -X POST \"http://wheelerlab.net/submit.php\" -s --data-binary \"@\"$jobname\".json\" -H \"Content-Type: text/xml\"\n",
        "  print(\"http://wheelerlab.net/alphafold/all/view.php?idse=\"+jobname)\n",
        "\n",
        "  plt.figure(figsize=(14,4),dpi=100)\n",
        "  plt.subplot(1,2,1); plt.title(\"Sequence coverage\")\n",
        "  plt.imshow(non_gaps[seqid_sort]*seqid[seqid_sort,None],\n",
        "             interpolation='nearest', aspect='auto',\n",
        "             cmap=\"rainbow_r\", vmin=0, vmax=1, origin='lower')\n",
        "  plt.plot((msa_arr != \"-\").sum(0), color='black')\n",
        "  plt.xlim(-0.5,msa_arr.shape[1]-0.5)\n",
        "  plt.ylim(-0.5,msa_arr.shape[0]-0.5)\n",
        "  plt.colorbar(label=\"Sequence identity to query\",)\n",
        "  plt.xlabel(\"Positions\")\n",
        "  plt.ylabel(\"Sequences\")\n",
        "\n",
        "  plt.subplot(1,2,2); plt.title(\"Predicted lDDT per position\")\n",
        "  for model_name,value in outs.items():\n",
        "    plt.plot(value[\"plddt\"],label=model_name)\n",
        "  if homooligomer > 0:\n",
        "    for n in range(homooligomer+1):\n",
        "      x = n*(len(query_sequence)-1)\n",
        "      plt.plot([x,x],[0,100],color=\"black\")\n",
        "  plt.legend()\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel(\"Predicted lDDT\")\n",
        "  plt.xlabel(\"Positions\")\n",
        "  #plt.savefig(jobname+\"_coverage_lDDT.png\")\n",
        "\n",
        "  plt.figure(figsize=(3*num_models,2), dpi=100)\n",
        "  for n,(model_name,value) in enumerate(outs.items()):\n",
        "    plt.subplot(1,num_models,n+1)\n",
        "    plt.title(model_name)\n",
        "    plt.imshow(value[\"pae\"],label=model_name,cmap=\"bwr\",vmin=0,vmax=30)\n",
        "    plt.colorbar()\n",
        "  #plt.savefig(jobname+\"_PAE.png\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
